# Matt x Unknown Brain Test Meet

**Creator:** Matt (matt@weareunknown.io)
**Date:** 2025-09-16T15:45:00+01:00
**Calendar Event ID:** 2ni0j80pka7nj6vhiv6gq56od3
**Meeting Link:** https://notes.granola.ai/d/667a971f-a106-472e-96d2-fe3066cd0a32
**Attendees:** email: mtoland96@gmail.com
name: Mtoland96
**File Created Timestamp:** 1758548485
**Zapier Step ID:** 667a971f-a106-472e-96d2-fe3066cd0a32

## Enhanced Notes
### Unknown Brain Project Overview

- Building searchable knowledge store for client conversations and market intel
- Solves consultant time waste hunting through folders for hiring/salary data
- Target outcome: reduce research time from hours to minutes
- Sample query: â€œWhat did we hear last quarter about London mid-senior product manager salary expectations in fintech?â€
  - System returns short answer plus 2-3 supporting transcript snippets with source links

### Technical Pipeline Architecture

- Flow: Granola â†’ Google Drive â†’ Cloud Run â†’ GPT-4 â†’ BigQuery â†’ Pinecone vectors â†’ Next.js on Vercel
- Data processing:
  - Export Granola transcripts as raw JSON
  - Lightweight ETL cleaning (remove speaker noise, normalize timestamps)
  - Chunk into 200-400 tokens with metadata (meeting ID, speaker, timestamp, client tags)
  - Vector embeddings stored in Pinecone
- Frontend: Next.js chat interface with search and PDF export functionality
- Expected volume: 200-300 meetings per month

### Cost & Risk Assessment

- Development: Â£9-13k for discovery + 8 weeks hybrid nearshore development
- Infrastructure: Â£166-350/month (Pinecone, GPT queries, Cloud Run, Vercel)
- Main risks:
  - Transcript quality and speaker diarization errors
  - PII compliance issues with contracts/candidate data
  - Scope creep toward analytics dashboards
- Mitigations: automated NER tagging, PII filtering, strict MVP scope (Q&A and briefings only)

### Discovery Sprint Deliverables

- Deliverable A: ingestion spec + ETL script (Python) with sample cleaning
- Deliverable B: proof-of-concept retrieval using 50 transcripts (index + query)
- Deliverable C: short demo chat UI wired to the POC
- Deliverable D: risk register and compliance checklist
- Success criteria: retrieval relevance >75% on 20 test queries (manual eval), demo UI can return answers with citations, ETL reliably removes PII on sample set

---

Chat with meeting transcript: [https://notes.granola.ai/d/667a971f-a106-472e-96d2-fe3066cd0a32?source=zapier](https://notes.granola.ai/d/667a971f-a106-472e-96d2-fe3066cd0a32?source=zapier)

## My Notes
-

For discovery sprint (2 weeks):

- Deliverable A: ingestion spec + ETL script (python) with sample cleaning.
- Deliverable B: proof-of-concept retrieval using 50 transcripts (index + query).
- Deliverable C: short demo chat UI wired to the POC.
- Deliverable D: risk register and compliance checklist.

Success criteria: retrieval relevance > 75% on 20 test queries (manual eval), demo UI can return answers with citations, and ETL reliably removes PII on sample set.

## Full Transcript
 
Me: Okay. So we're both online. Let's continue to talk about the unknown brain and what we know too. Date. Okay. So we're doing a unknown brain granola only, so we're ingesting only the granola midi note from a shared Google Drive folder and deliver chat search with citations. So the flow will be Granola to Google Drive to Cloud Run to GPT four to BigQuery warehouse to Pinecone vectors and then NXJS on Vercel up with a UI and API. Infrastructure cost will be about a Â£166 per month. That's a pine cone by query, GP extraction, cloud run, Versaill. Why it's low risk? There's no ATS, no client portal, just meeting notes select strict citations, redaction for PII. Before embedding Hi, everyone. Thanks for joining. Quick sound check. Can you all hear me clearly? Great. I'm going to run through a short agenda. One line problem statement, proposed pipeline for Granola knowledge store, chat. Blockers, and calls, ballpark, immediate next steps, and then I'll stop for questions after each section. Okay. One line problem. Unknown needs a searchable, auditable brain of client conversations on market intel so consultants can quickly answer hiring and salary questions. Spot trends, and produce client briefings without hunting through folders. The user outcome reduced research time from hours to minutes and improved proposal accuracy. That resonates. Do you have an example question we want the brand handle on day one? Yep. Sample question. What did we last hear? What did we hear last quarter about London mid senior product manager salary expectations in fintech? Ideally, the system would return a short answer plus two or three supporting transcripts submits and a source link. Like a meeting ID or a file name. Okay. Nice. That looks good to me. I'm happy to go forward with that and look at KPIs from here on in. What about the pipeline? Yeah. I can Yes. The proposed pipeline would be a simple rag first pass. Granola transcripts export to raw JSON, then a lightweight ETL. Clean the text, remove the speaker noise, Time stop normalization, detect where agendas decisions occur, save raw and clean to cloud storage at GCS. Then we chunk, so 200 to 400 token chunks with metadata, meeting ID, speaker, timestamp, client, keywords, tags, role, candidate, client, that kind of thing. Then we do an embedding store in a vector database like pine cone. We'd use retrieval on an LLM layer for a query retriever returns top k chunks. An LLM prompt template synthesizes the answer and includes citations with the meeting ID, the time stamp, and the confidence. The front end would be a Next. Js chat with search create briefing button, and expect export to PDF. Okay. So for tagging, are we doing automated tags or manual do we handle PII? Great question. Basically, we start with automated NER tags. Company names, roles, salary mentions. PII filter wise, we strip the phone numbers and emails through an ETL and store hash identifiers for an agreed Manual review, UI optional later, but not in MVP. Okay. Great. What about costs? Let's look at that. And from the engineering side, what's the expected throughput? How many meetings per week, that kind of Yeah. So ballpark, I would say for the unknown initial scope, 200 to 300 meetings per month. That's why we keep chunk size and embed cost in mind. As for the cost, the ballpark for the MVP bill, discovery plus eight weeks dev, nine to 13,000 pounds of hybrid near shore. Recurring cloud ops may be a 180 to three fifty per month depending on embedding volume and hosting for testing. You can throttle embed embeddings into a rolling backfill. Great. What are the main blockers you foresee? Three top risks transcript quality, speaker diarization errors, sensitive data, and compliance contradicts contracts and candidate data, and scope creep client ask for analytics dashboards, not just search mitigations doing initial quality check script, build PII scrub rules, unlock MVPs scope to q and a and briefings. Okay. Great. Is anyone everyone happy with that? That all sounds good, we can leave it there today. Thanks, folks. 